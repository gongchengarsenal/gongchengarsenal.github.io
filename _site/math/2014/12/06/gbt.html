<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Gradienting Boosting Tree</title>
        
        <link href="/css/bootstrap.min.css" rel="stylesheet">
        <link href="/css/my.blog.css" rel="stylesheet">
        <style id="holderjs-style" type="text/css"></style> 
    </head>
    
    <body>
        <div class="blog-masthead">
            <div class="container">
                <nav class="blog-nav">
                    <a class="blog-nav-item" href="/index.html">Home</a>
                    <a class="blog-nav-item" href="/math/index.html">Math</a>
                    <a class="blog-nav-item" href="/coding/index.html">Coding</a>
                    <a class="blog-nav-item" href="/gallery/index.html">Gallery</a>
                    <a class="blog-nav-item" href="/resume/index.html">Resume</a>
                </nav>
            </div>
        </div>
        <div class="container">
            <div class="blog-header">
                <h1 class="blog-title">Gong's Page</h1>
<p class="lead blog-description">stay hungry, stay foolish</p>

            </div>
            <div class="row">
                <div class="col-sm-8 blog-main">
                    <div class="blog-post">
                    <h2 class="blog-post-title">Gradienting Boosting Tree</h2>
                    <p class="blog-post-meta">December 06 2014</p>
                    <p><p style="text-align:justify;">Gradient boosting tree is a learning machine which produces an additive expension with regression trees as the basis function. Gradient boosting tree builds the model iteratively by fitting a single tree at each step like other boosting methods do, and it generalizes them by allowing arbitrary differentiable loss function.</p>
<div class="blog-post-contents">
<ul>
	<li><a href="#bst">Boosting method</a></li>
	<li><a href="#gbst">Gradient boosting</a></li>
	<li><a href="#imp">Implement a <span class="caps">GBT</span></a><br />
</div></li>
</ul>
<h3 id=bst>Boosting Method</h3>
<p style="text-align:justify;">Boosting is believed to be one of the most powerful learning method. The motivation is to combine many &#8220;weak learners&#8221; to produce a powerful &#8220;committee&#8221;. From this perspective, the boosting method seems to bear a resemblance to bagging or other committee-based techniques. However we shall see that boosting is fundamentally different.</p>
<p style="text-align:justify;">The key of boosting method lies in the fact that it fits an additive expension in a forword stagewise fashion. Generally, the additive expension has the form:</p>
<p><img src="/images/gbt/Screen.png" style="display: block;margin-left: auto;margin-right: auto;width: 30%;height: auto;" alt="" /></p>
<p style="text-align:justify;">where <img src="http://latex.codecogs.com/gif.latex?\beta&amp;space;_{m},&amp;space;m=1,2,..,M" alt="" /> are expension coefficients, and <img src="/images/gbt/CodeCogsEqn-2.gif" alt="" /> is usually a simple function of multivariate argument <i>x</i>. Typically the model is fit by minimizing a loss function over the training data:</p>
<p><img src="/images/gbt/Screen-2.png" style="display: block;margin-left: auto;margin-right: auto;width: 50%;height: auto;" alt="" /></p>
<p style="text-align:justify;">which always, for many loss functions <img src="/images/gbt/CodeCogsEqn.gif" alt="" /> and basis functions <img src="/images/gbt/CodeCogsEqn-2.gif" alt="" />, requires intensive numerical computation. So the boosting method works to make the computation affordable by rapidly fitting just a single basis function at each time, and add new basis function to the expension without adjusting the previous ones. The procedures can be described as:</p>
<p><img src="/images/gbt/Screen-3.png" style="display: block;margin-left: auto;margin-right: auto;width: 80%;height: auto;" alt="" /></p>
<p style="text-align:justify;">Generally, we let a regression tree be the basis function. The regression tree has the formal expression:</p>
<p><img src="/images/gbt/Screen-4.png" style="display: block;margin-left: auto;margin-right: auto;width: 40%;height: auto;" alt="" /></p>
<p style="text-align:justify;">with parameters <img src="http://latex.codecogs.com/gif.latex?\Theta&amp;space;=&amp;space;\left&amp;space;\{&amp;space;R_{j},\gamma&amp;space;_{j}&amp;space;\right&amp;space;\}" alt="" />, where <img src="http://latex.codecogs.com/gif.latex?R_{j}" alt="" /> are <i>J</i> regions of the space of all joint variables and <img src="http://latex.codecogs.com/gif.latex?\gamma&amp;space;_{j}" alt="" /> are output values assigned to each region. So the 2(a) step is rewrite as:</p>
<p><img src="/images/gbt/Screen-5.png" style="display: block;margin-left: auto;margin-right: auto;width: 60%;height: auto;" alt="" /></p>
<p style="text-align:justify;">When the regions <img src="http://latex.codecogs.com/gif.latex?R_{j}" alt="" /> are given, searching the optimal constant values <img src="http://latex.codecogs.com/gif.latex?\gamma&amp;space;_{j}" alt="" /> is quite straightforward:</p>
<p><img src="/images/gbt/Screen-6.png" style="display: block;margin-left: auto;margin-right: auto;width: 58%;height: auto;" alt="" /></p>
<p style="text-align:justify;">However finding the regions is much more difficult. For some special cases, e.g. square loss and exponential loss, this problem simplifies, and is even no harder than finding the regions for a single tree. Take the square loss for instance, one has:</p>
<p><img src="http://latex.codecogs.com/gif.latex?L\left&amp;space;(&amp;space;y_{i},&amp;space;f_{m-1}\left&amp;space;(&amp;space;x_{i}&amp;space;\right&amp;space;)&amp;space;&amp;plus;T\left&amp;space;(&amp;space;x_{i};\Theta&amp;space;_{m}&amp;space;\right&amp;space;)\right&amp;space;)&amp;space;=&amp;space;\sum_{i=1}^{N}\left&amp;space;(&amp;space;y_{i}&amp;space;-&amp;space;f_{m-1}\left&amp;space;(&amp;space;x_{i}&amp;space;\right&amp;space;)&amp;space;-T\left&amp;space;(&amp;space;x_{i};\Theta&amp;space;_{m}&amp;space;\right&amp;space;)&amp;space;\right&amp;space;)^{2}" style="display: block;margin-left: auto;margin-right: auto;width: 83%;height: auto;" alt="" /></p>
<p style="text-align:justify;">So at each step, we simply build a regression tree to fit the current residuals over training data. Nevertheless, some other loss criteria such as absolute error or deviance loss does not give rise to a fast boosting algorithm. The Gradient boosting is a simple alternative to solve the problem of this case.</p>
<h3 id=gbst>Gradient Boosting</h3>
<p style="text-align:justify;">Gradient boosting is a functional gradient descent view of boosting. Given loss function <i>L</i>, the goal is to minimize <i>L(y, f)</i> with respect to <i>f</i>, where <i>f(x)</i> is the additive expension which is constrained to be a sum of regression trees. For a differentiable L, the gradient descent method works to minimize <i>L(y, f)</i> by calculating the gradient of L respect to f, and updating f in the negative gradient direction. As the values of gradient can only be measured at the observed points, we transfer the problem from the function space to a N-dimension vector space, where</p>
<p><img src="/images/gbt/Screen-7.png" style="display: block;margin-left: auto;margin-right: auto;width: 45%;height: auto;" alt="" /></p>
<p><img src="/images/gbt/Screen-8.png" style="display: block;margin-left: auto;margin-right: auto;width: 30%;height: auto;" alt="" /></p>
<p style="text-align:justify;">Than the step size is decided by line search:</p>
<p><img src="/images/gbt/Screen-10.png" style="display: block;margin-left: auto;margin-right: auto;width: 42%;height: auto;" alt="" /></p>
<p style="text-align:justify;">The component of gradient vector is:</p>
<p><img src="/images/gbt/Screen-9.png" style="display: block;margin-left: auto;margin-right: auto;width: 45%;height: auto;" alt="" /></p>
<p style="text-align:justify;">The gradient is defined only at the observed points, whereas our ultimate goal is to generalize <i>f(x)</i> to the future data. A possible solution to this dilemma is to induce a regression tree at each step whose prediction is as close as possible to the negative gradient. That is to say, we fit a tree to the negative gradient with square error as the criteria:</p>
<p><img src="/images/gbt/Screen-11.png" style="display: block;margin-left: auto;margin-right: auto;width: 50%;height: auto;" alt="" /></p>
<p style="text-align:justify;">Instead of performing a seperate line search in deepest descent, we find the regions <img src="http://latex.codecogs.com/gif.latex?R_{j}" alt="" /> by squere error criteria, and then decide the <img src="http://latex.codecogs.com/gif.latex?\gamma&amp;space;_{j}" alt="" /> within each region by the given loss function <i>L</i>:</p>
<p><img src="/images/gbt/Screen-12.png" style="display: block;margin-left: auto;margin-right: auto;width: 85%;height: auto;" alt="" /></p>
<p style="text-align:justify;">In the two-class classification problem, the response is Bernoulli distributed, so we prefer to take the logit model with the form:</p>
<p><img src="/images/gbt/Screen-13.png" style="display: block;margin-left: auto;margin-right: auto;width: 38%;height: auto;" alt="" /></p>
<p style="text-align:justify;">and fit it by the deviance loss:</p>
<p><img src="/images/gbt/Screen-14.png" style="display: block;margin-left: auto;margin-right: auto;width: 60%;height: auto;" alt="" /></p>
<p style="text-align:justify;">Accordingly, the pseudo-residual in 2(a) is:</p>
<p><img src="/images/gbt/CodeCogsEqn-4.gif" style="display: block;margin-left: auto;margin-right: auto;width: 47%;height: auto;" alt="" /></p>
<p style="text-align:justify;">and the Newton-Raphson method gives a numerical solution to <img src="http://latex.codecogs.com/gif.latex?\gamma&amp;space;_{jm}" alt="" />:</p>
<p><img src="/images/gbt/CodeCogsEqn-5.gif" style="display: block;margin-left: auto;margin-right: auto;width: 40%;height: auto;" alt="" /></p>
<h3 id=imp>Implement a <span class="caps">GBT</span></h3>
<p style="text-align:justify;">There are three primary parameters in building a gradient boosting tree: tree size, the number of trees and shrinkage. Since a tree can be viewed as a partition of the space of joint variables, the number of leaf nodes limits the degree to which variables interact with each other. For a tree with <i>J</i> leaf nodes, interaction of level higher than <i>J-1</i> is not allowed. Generally a value of 4 ~ 8 is good enough. Each iteration usually reduces the training risk, so that for <i>M</i> large enough the risk can be arbitrary small, which necessarily leads to overfitting on the training data. There is a convinent way to estimate the optimal <i>M <sup>*</sup></i> minimizing the future risk. We can monitor the prediction loss on validation data as function of <i>M</i>, and use the early stopping strategy to decide when to stop. Shrinkage is another way to regularize the model. Each time a new tree added to the model, we always impose a shrinkage factor on it, which controls the rate that function <i>f</i> &#8220;moves&#8221; in the negative gradient direction. A very small value (&lt;0.1) always yields models with better generalization ability.</p>
<p style="text-align:justify;">When you write a <span class="caps">GBT</span> software, there are several considerations worth your attention. Finding the optimal regions <img src="http://latex.codecogs.com/gif.latex?R_{j}" alt="" /> is the most time-consuming step in building a regression tree. Each time we split an interior node of the tree, we consider a splitting variable <i>j</i> and split point <i>s</i>, and define the half-planes:</p>
<p><img src="/images/gbt/Screen-15.png" style="display: block;margin-left: auto;margin-right: auto;width: 63%;height: auto;" alt="" /></p>
<p style="text-align:justify;">Then we seek the best binary partition that solves</p>
<p><img src="/images/gbt/Screen-16.png" style="display: block;margin-left: auto;margin-right: auto;width: 70%;height: auto;" alt="" /></p>
<p style="text-align:justify;">For any choice of <i>(j,s)</i>, the inner minimization is achieved at</p>
<p><img src="/images/gbt/Screen-17.png" style="display: block;margin-left: auto;margin-right: auto;width: 75%;height: auto;" alt="" /></p>
<p style="text-align:justify;">so actually we are minimizing the sum of variance within two half-planes. Given a variable <i>j</i>, this can be done by sorting the data points by the value of variable <i>j</i> and scanning through all of them. The sufficient statistics for calculating variance, which consists of <img src="http://latex.codecogs.com/gif.latex?\inline&amp;space;\sum_{i\in&amp;space;R_{1}\left&amp;space;(&amp;space;j,s&amp;space;\right&amp;space;)}\gamma&amp;space;_{im}" alt="" />, <img src="http://latex.codecogs.com/gif.latex?\inline&amp;space;\sum_{i\in&amp;space;R_{2}\left&amp;space;(&amp;space;j,s&amp;space;\right&amp;space;)}\gamma&amp;space;_{im}" alt="" />, <img src="http://latex.codecogs.com/gif.latex?\inline&amp;space;\sum_{i\in&amp;space;R_{1}\left&amp;space;(&amp;space;j,s&amp;space;\right&amp;space;)}\gamma_{im}^{2}" alt="" /> and <img src="http://latex.codecogs.com/gif.latex?\inline&amp;space;\sum_{i\in&amp;space;R_{2}\left&amp;space;(&amp;space;j,s&amp;space;\right&amp;space;)}\gamma_{im}^{2}" alt="" /> are updated as the scanning proceeds. The time complexity is <i>O(nlogn)</i>. Obviousely, searching the best splitting point of a certain variable is independent to that of another, hence parallelization shall dramtically accelerate this procedure.</p>
<p style="text-align:justify;">Another trick further improves the time performance at the cost of negligible accuracy loss. Suppose the value range of each variable was divided into <i>k</i> bins beforehand, we can preprocess the training data by mapping original values to corresponding bins. To find a splitting point <i>s</i> for variable <i>j</i>, we simply parse through the binned values and collect the sufficient statistics of each bin. Without the sorting step, time complexity is reduced to <i>O(n)</i>. Though many discretization technique can be used to decide the bins, a test shows that the accuracy is not as sensitive as one may expect to the choice of bins. The quantile is preferable in most cases for its good performance and cheap computation. What&#8217;s more, this strategy allows a distributed tree algorithm. A single-machine version <span class="caps">GBT</span> in Java and a distributed one based on Spark are available in <a href="https://github.com/gongchengarsenal">my Git</a>.</p></p>
                    </div>
                </div>
                <div class="col-sm-3 col-sm-offset-1 blog-sidebar">
                    <div class="sidebar-module sidebar-module-inset">
    <h4>About Me</h4>
    <div class="portrait">
        <img src="/images/photo.JPG" width="150" />
    </div>
    <p>A programmer living in Beijing, keen on coding, mathematics and rock music.</p>
</div>

<div class="sidebar-module">
    <h4>Archives</h4>
    
        
        
        
            <li><a href="#">January, 2015</a></li>
        
    
        
        
        
            <li><a href="#">December, 2014</a></li>
        
    
</div>

                </div>
            </div>
        </div>
    </body>
</html>
