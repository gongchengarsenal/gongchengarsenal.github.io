<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>EM Algorithm and Its Applications</title>
        
        <link href="/css/bootstrap.min.css" rel="stylesheet">
        <link href="/css/my.blog.css" rel="stylesheet">
        <style id="holderjs-style" type="text/css"></style> 
    </head>
    
    <body>
        <div class="blog-masthead">
            <div class="container">
                <nav class="blog-nav">
                    <a class="blog-nav-item" href="/index.html">Home</a>
                    <a class="blog-nav-item" href="/math/index.html">Math</a>
                    <a class="blog-nav-item" href="/coding/index.html">Coding</a>
                    <a class="blog-nav-item" href="/gallery/index.html">Gallery</a>
                    <a class="blog-nav-item" href="/resume/index.html">Resume</a>
                </nav>
            </div>
        </div>
        <div class="container">
            <div class="blog-header">
                <h1 class="blog-title">Gong's Page</h1>
<p class="lead blog-description">stay hungry, stay foolish</p>

            </div>
            <div class="row">
                <div class="col-sm-8 blog-main">
                    <div class="blog-post">
                    <h2 class="blog-post-title">EM Algorithm and Its Applications</h2>
                    <p class="blog-post-meta">January 10 2015</p>
                    <p><p style="text-align:justify;">The expectation–maximization (EM) algorithm is an iterative method to simplify a maximum likelihood problem which is difficult to solve directly. Firstly, we describe a simple Gaussian mixture model to bring forward the generalized EM algorithm, and than show some frequent applications of this algorithm.</p>
<div class="blog-post-contents">
<ul>
	<li><a href="#gauss">Mixture Gaussian model</a></li>
	<li><a href="#gem">Generalized EM algorithm</a></li>
	<li><a href="#kmeans">K-means algorithm</a></li>
	<li><a href="#hmm">Hidden Markov model</a><br />
</div></li>
</ul>
<h3 id=gauss>Mixture Gaussian Model</h3>
<p><img src="/images/em/Screen.png" style="display: block;margin-left: auto;margin-right: auto;width: 30%;height: auto;" alt="" /></p>
<p style="text-align:justify;">The figure above shows the histogram of 20 data points. We’ d like to model density of these data points, and obviously, a Gaussian distribution would not be appropriate. Instead, we model the data with a mixture of two normal distributions:</p>
<p><img src="/images/em/CodeCogsEqn.gif" style="display: block;margin-left: auto;margin-right: auto;" alt="" /><br />
<img src="/images/em/CodeCogsEqn-2.gif" style="display: block;margin-left: auto;margin-right: auto;" alt="" /><br />
<img src="/images/em/CodeCogsEqn-3.gif" style="display: block;margin-left: auto;margin-right: auto;" alt="" /></p>
<p style="text-align:justify;">where <img src="http://latex.codecogs.com/gif.latex?\inline&amp;space;\Delta&amp;space;\in&amp;space;\left&amp;space;\{&amp;space;0,1&amp;space;\right&amp;space;\}" alt="" /> with <img src="/images/em/CodeCogsEqn-4.gif" alt="" />, so the parameters of this model is <img src="/images/em/CodeCogsEqn-5.gif" alt="" />, which gives the form of log-likelihood:</p>
<p><img src="/images/em/CodeCogsEqn-6.gif" style="display: block;margin-left: auto;margin-right: auto;" alt="" /></p>
<p style="text-align:justify;">Direct maximization is numerically difficult, so we consider latent variables <img src="http://latex.codecogs.com/gif.latex?\Delta&amp;space;_{i}" alt="" /> indicating which of two normal distributions a data points is from. Than the log-likelihood can be rewrite as:</p>
<p><img src="/images/em/CodeCogsEqn-7.gif" style="display: block;margin-left: auto;margin-right: auto;" alt="" /><br />
<img src="/images/em/CodeCogsEqn-8.gif" style="display: block;margin-left: auto;margin-right: auto;" alt="" /></p>
<p style="text-align:justify;">Since the values of latent variables are actually unknown, we substitute each of them with the value of condition expectation under current estimates of the parameters, which could be viewed as a soft assignment of the observations to each model. In the next step, with these values, maximization of log-likelihood is trivial, and the estimates of the parameters are updated subsequently. The formulation of this algorithm is given as follows:</p>
<p><img src="/images/em/Screen-2.png" style="display: block;margin-left: auto;margin-right: auto;width: 90%;height: auto;" alt="" /></p>
<p style="text-align:justify;">There exists a good way to construct the initial guess of the parameters. <img src="http://latex.codecogs.com/gif.latex?\mu&amp;space;_{1}" alt="" /> and <img src="http://latex.codecogs.com/gif.latex?\mu&amp;space;_{2}" alt="" /> can be two of the <img src="http://latex.codecogs.com/gif.latex?y" alt="" /> chosen at random; <img src="http://latex.codecogs.com/gif.latex?\sigma&amp;space;_{1}^{2}" alt="" /> and <img src="http://latex.codecogs.com/gif.latex?\sigma&amp;space;_{2}^{2}" alt="" /> can equals to the overall sample variance, and <img src="http://latex.codecogs.com/gif.latex?\pi" alt="" /> can be started at 0.5.</p>
<h3 id=gem>Generalized EM Algorithm</h3>
<p style="text-align:justify;">The procedure above is a instance of EM algorithm for maximizing likelihood in the case that direct maximization of the likelihood is difficult, but made easier by enlarge the sample with latent data. Table 1.2 gives the general formulation of EM algorithm:</p>
<p><img src="/images/em/Screen-3.png" style="display: block;margin-left: auto;margin-right: auto;width: 90%;height: auto;" alt="" /></p>
<p style="text-align:justify;">where <img src="http://latex.codecogs.com/gif.latex?Z" alt="" /> is the observed data, having log-likelihood <img src="/images/em/CodeCogsEqn-9.gif" alt="" /> depending on parameters <img src="http://latex.codecogs.com/gif.latex?\theta" alt="" />. The latent variable is <img src="http://latex.codecogs.com/gif.latex?Z^{m}" alt="" />, so that <img src="/images/em/CodeCogsEqn-10.gif" alt="" /> is the complete data with log-likelihood <img src="/images/em/CodeCogsEqn-11.gif" alt="" /></p>
<p style="text-align:justify;">In the E step, according to Bayesian theory</p>
<p><img src="/images/em/Screen-4.png" style="display: block;margin-left: auto;margin-right: auto;width: 30%;height: auto;" alt="" /></p>
<p style="text-align:justify;">we have <img src="/images/em/CodeCogsEqn-12.gif" alt="" />. As the actual value of latent variable is unknown, we replace the probabilities with corresponding conditional expectation with respect to T|Z and current estimates of parameters, so the log-likelihood has the form:</p>
<p><img src="/images/em/Screen-5.png" style="display: block;margin-left: auto;margin-right: auto;width: 65%;height: auto;" alt="" /></p>
<p style="text-align:justify;">In the M step, we determine new estimates of the parameters by maximizing <img src="http://latex.codecogs.com/gif.latex?\inline&amp;space;E[l_{0}(\theta&amp;space;^{&#39;};T)|Z,\theta&amp;space;]" alt="" />, and the iteration carries on until convergence achieved. The key of this algorithm is it works to iteratively improve the log-likelihood based on the complete density, say <img src="http://latex.codecogs.com/gif.latex?\inline&amp;space;Q(\theta&amp;space;^{&#39;}|\theta&amp;space;)=E[l_{0}(\theta&amp;space;^{&#39;};T)|Z,\theta&amp;space;]" alt="" />, rather than directly improves the actual log-likelihood based on observed data, and the correctness is justified <a href="https://en.wikipedia.org/wiki/Expectation–maximization_algorithm#Proof_of_correctness">here</a>.</p>
<h3 id=kmeans>K-Means Algorithm</h3>
<p style="text-align:justify;">However, it is possible to apply EM in another way, and the motivation is as follows. For a given estimate of the parameters, we can usually fine a best value of latent variable Z, by maximizing the likelihood, over all possible values of Z. Conversely, with these values settled, we can group the data points according to the values of associated latent data, which makes maximization of log-likelihood fairly easy. The k-means algorithm is a example of EM algorithm with this settings.</p>
<p style="text-align:justify;">Given a set of data points, k-means algorithm aims to group them into k sets, so as to minimise the within-cluster sum of squares:</p>
<p><img src="https://upload.wikimedia.org/math/9/8/3/983406139b111b6676a3db71cc217f2c.png" style="display: block;margin-left: auto;margin-right: auto;width: 30%;height: auto;" alt="" /></p>
<p style="text-align:justify;">Since the sum of squares is squared Euclidean distance, k-means procedure is closely related to the Gaussian mixture model above. In the E step, we’d rather make a hard choice of Z given the current estimate of parameters than assign a “responsibilities” of each component to each data point. Corresponding with this, we average the values of observed data in new cluster, which determined by the value of Z, to obtain the new estimate of the parameters in M step. The details of assignment (E) step and update (M) step are given as fallows:</p>
<p><img src="https://upload.wikimedia.org/math/5/6/1/56171499da5ae8958743d8a193a4436e.png" style="display: block;margin-left: auto;margin-right: auto;width: 65%;height: auto;" alt="" /><br />
<img src="https://upload.wikimedia.org/math/a/2/f/a2f6a26828c150b6d266d230b9b57e37.png" style="display: block;margin-left: auto;margin-right: auto;width: 30%;height: auto;" alt="" /></p>
<p style="text-align:justify;">Likewise, the convergence value is sensitive to the start <img src="http://latex.codecogs.com/gif.latex?\left&amp;space;\{&amp;space;m_{1}^{\left&amp;space;(&amp;space;1&amp;space;\right&amp;space;)},m_{2}^{\left&amp;space;(&amp;space;1&amp;space;\right&amp;space;)},...m_{k}^{\left&amp;space;(&amp;space;1&amp;space;\right&amp;space;)}&amp;space;\right&amp;space;\}" alt="" />. Some variations (e.g. <a href="https://en.wikipedia.org/wiki/K-means%2B%2B">k-means++</a>) work to alleviate this problem.</p>
<h3 id=hmm>Hidden Markov Model</h3>
<p style="text-align:justify;">A hidden Markov model (<span class="caps">HMM</span>) is a special form of Markov model in which the state is not directly visible, but the output, dependent on the state, is visible. Naturally, this problem with latent variable can be associated with EM algorithm. There are several inference problems about hidden Markov model. This article discusses the parameter learning task which can be solved by the Baum-welch algorithm, a special case of EM algorithm.</p>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/8/83/Hmm_temporal_bayesian_net.svg" style="display: block;margin-left: auto;margin-right: auto;width: 60%;height: auto;" alt="" /></p>
<p style="text-align:justify;">To learn the parameters of <span class="caps">HMM</span> is to derive the maximum likelihood estimate of them for a given set of output sequences. Since the output absolutely depends on the state, we introduce the state sequences as latent data to simplify maximization of the likelihood. A hidden Markov model shares with the Markov chain the assumption that the i <sup>th</sup> hidden variable, given the (i-1) <sup>th</sup> hidden variable, is independent of all previous hidden variables and time, and current output depends only on current hidden variable. With this assumption, we can describe hidden Markov chain like this:</p>
<p style="text-align:justify;">A initial state distribution</p>
<p><img src="https://upload.wikimedia.org/math/f/c/f/fcff600fb1401c401c20b9c33fe4a4f6.png" style="display: block;margin-left: auto;margin-right: auto;width: 20%;height: auto;" alt="" /></p>
<p style="text-align:justify;">A time independent stochastic transition matrix</p>
<p><img src="https://upload.wikimedia.org/math/5/0/8/5080146300de6f45e122a2bfca116099.png" style="display: block;margin-left: auto;margin-right: auto;width: 42%;height: auto;" alt="" /></p>
<p style="text-align:justify;">The probability of a certain observation at time t for state j is given</p>
<p><img src="https://upload.wikimedia.org/math/e/6/0/e600277f67923fbff1addc488644dd23.png" style="display: block;margin-left: auto;margin-right: auto;width: 35%;height: auto;" alt="" /></p>
<p style="text-align:justify;">Usually we employ the Baum–Welch algorithm to find local optimal of the parameters that maximize the likelihood, that is <img src="https://upload.wikimedia.org/math/2/8/9/2898c706da66d4d3ac6b1a68d6cb70cc.png" alt="" />, where <img src="https://upload.wikimedia.org/math/c/7/d/c7dd4f31286130346ca928cd8460a6d0.png" alt="" />. Suppose we do know the values of latent variables, say the state <img src="http://latex.codecogs.com/gif.latex?\inline&amp;space;\left&amp;space;\{&amp;space;X_{1},X_{2},...X_{T}&amp;space;\right&amp;space;\}" alt="" />, we can estimate the parameters fairly easily:</p>
<p><img src="http://latex.codecogs.com/gif.latex?a_{ij}^{*}=\frac{count\left&amp;space;\{&amp;space;X_{t}=j,X_{t-1}=i&amp;space;\right&amp;space;\}}{count\left&amp;space;\{&amp;space;X_{t-1}=i&amp;space;\right&amp;space;\}}" style="display: block;margin-left: auto;margin-right: auto;width: 40%;height: auto;" alt="" /></p>
<p style="text-align:justify;">and</p>
<p><img src="http://latex.codecogs.com/gif.latex?b_{i}^{*}\left&amp;space;(&amp;space;k&amp;space;\right&amp;space;)=\frac{count\left&amp;space;\{&amp;space;y_{t}=k,X_{t}=i&amp;space;\right&amp;space;\}}{count\left&amp;space;\{&amp;space;X_{t}=i&amp;space;\right&amp;space;\}}" style="display: block;margin-left: auto;margin-right: auto;width: 40%;height: auto;" alt="" /></p>
<p style="text-align:justify;">In light of this, we give the conditional probability mass function of the latent variable respect to observed data and current estimate of parameters, than derive a new estimate with the expected number of each transitions and the times each state is observed. The details are as fallows:</p>
<p style="text-align:justify;"><i>Begining</i></p>
<p style="text-align:justify;">Set <img src="https://upload.wikimedia.org/math/c/7/d/c7dd4f31286130346ca928cd8460a6d0.png" alt="" /> with prior information about the parameters if there is any, otherwise a random value</p>
<p style="text-align:justify;"><i>Forward Procedure</i></p>
<p style="text-align:justify;">Let <img src="https://upload.wikimedia.org/math/1/3/e/13e03903d266321214826ba19224407f.png" alt="" />, the probability of seeing the <img src="https://upload.wikimedia.org/math/7/7/9/77925b05c167135bf34e19f07479b17b.png" alt="" /> and being in state <i>i</i> at time <i>t</i>, and</p>
<p><img src="https://upload.wikimedia.org/math/3/8/5/38575a3493ad507500ed22e9c6ef136e.png" style="display: block;margin-left: auto;margin-right: auto;" alt="" /></p>
<p><img src="https://upload.wikimedia.org/math/b/e/4/be435d15f1a189ec5f1e3c1fb09cfa77.png" style="display: block;margin-left: auto;margin-right: auto;" alt="" /></p>
<p style="text-align:justify;"><i>Backward Procedure</i></p>
<p style="text-align:justify;">Let <img src="https://upload.wikimedia.org/math/7/1/8/718391a46cbf71878cde7c3d353dccb5.png" alt="" />, the probability of the ending partial sequence <img src="https://upload.wikimedia.org/math/7/2/d/72d4f0fec008e2204f623fde18d2fd82.png" alt="" /> given starting state <i>i</i> at time <i>t</i>, and</p>
<p><img src="https://upload.wikimedia.org/math/9/d/0/9d09847ed057bcaa0b9f021077a61e5b.png" style="display: block;margin-left: auto;margin-right: auto;" alt="" /></p>
<p><img src="https://upload.wikimedia.org/math/b/1/4/b14acf93528a159ff2984882277f28a0.png" style="display: block;margin-left: auto;margin-right: auto;" alt="" /></p>
<p style="text-align:justify;"><i>Update</i></p>
<p style="text-align:justify;">After forward and backward procedures, we can now calculate the probability of being in state <i>i</i> at time <i>t</i> given the observed sequence <i>Y</i> and current estimate of parameters:</p>
<p><img src="https://upload.wikimedia.org/math/a/4/4/a44c6281ae3ed277060d27bac1f99f01.png" style="display: block;margin-left: auto;margin-right: auto;" alt="" /></p>
<p>and the probability of being in state <i>i</i> and <i>j</i> at times <i>t</i> and <i>(t+1)</i> respectively:</p>
<p><img src="https://upload.wikimedia.org/math/8/6/7/867226ec810c6bca9eec6a70bd1c30d9.png" style="display: block;margin-left: auto;margin-right: auto;width: 95%;height: auto;" alt="" /></p>
<p style="text-align:justify;">Than we can substitute <img src="http://latex.codecogs.com/gif.latex?count\left&amp;space;\{&amp;space;X_{t}=j,X_{t-1}=i&amp;space;\right&amp;space;\}" alt="" />, <img src="http://latex.codecogs.com/gif.latex?count\left&amp;space;\{&amp;space;y_{t}=k,X_{t}=i&amp;space;\right&amp;space;\}" alt="" /> and <img src="http://latex.codecogs.com/gif.latex?count\left&amp;space;\{X_{t}=i&amp;space;\right&amp;space;\}" alt="" /> with their expected values, which gives the new estimate of parameters:</p>
<p><img src="https://upload.wikimedia.org/math/4/f/3/4f360962316d11c840307ecbd7c0f470.png" style="display: block;margin-left: auto;margin-right: auto;" alt="" /></p>
<p><img src="https://upload.wikimedia.org/math/3/5/f/35fd6c44d7313a417d35aad2e1745f62.png" style="display: block;margin-left: auto;margin-right: auto;" alt="" /></p>
<p><img src="https://upload.wikimedia.org/math/7/9/c/79cd7245a0d95d2ee87b6675a3578b01.png" style="display: block;margin-left: auto;margin-right: auto;" alt="" /></p>
<p style="text-align:justify;">Unfortunately, Baum-Welch algorithm also can&#8217;t guarantee a global optimal due to the nature of EM algorithm.</p></p>
                    </div>
                </div>
                <div class="col-sm-3 col-sm-offset-1 blog-sidebar">
                    <div class="sidebar-module sidebar-module-inset">
    <h4>About Me</h4>
    <div class="portrait">
        <img src="/images/photo.JPG" width="150" />
    </div>
    <p>A programmer living in Beijing, keen on coding, mathematics and rock music.</p>
</div>

<div class="sidebar-module">
    <h4>Archives</h4>
    
        
        
        
            <li><a href="#">January, 2015</a></li>
        
    
        
        
        
            <li><a href="#">December, 2014</a></li>
        
    
</div>

                </div>
            </div>
        </div>
    </body>
</html>
